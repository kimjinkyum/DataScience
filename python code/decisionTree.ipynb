{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "data=pd.read_csv(\"../data/decision_tree_data.csv\",dtype=\"string_\")\n",
    "count_column=int(len(data)*(9/10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitRandom function\n",
    "# description : split data randomly \n",
    "def splitRandom(origin):\n",
    "    list_all=np.arange(len(origin)).tolist()\n",
    "    #randomly select amount of count in 0~ length of data(index of training) \n",
    "    list_training= random.sample(list_all, count_column)\n",
    "    \n",
    "    list_testing=[]#array store index of testing\n",
    "\n",
    "    #test, training index are not duplicate\n",
    "\n",
    "    for i in range(len(list_all)):\n",
    "        for j in range(len(list_training)):\n",
    "            if i==list_training[j]:\n",
    "                break;\n",
    "            else:\n",
    "                if j==len(list_training)-1:\n",
    "                    list_testing.append(i)\n",
    "                \n",
    "    \n",
    "     #split data train, test using index\n",
    "    data_training=origin.drop(list_testing)\n",
    "    data_testing=origin.drop(list_training);\n",
    "\n",
    "    return data_training, data_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(data, originaldata, features, target, parent_node_class=None):\n",
    "    \n",
    "\n",
    "    #Unique value of target data, number of each.\n",
    "    elements, count=np.unique(data[target],return_counts=True)\n",
    "   \n",
    "  \n",
    "    #attribute has single value (==child node is label data)\n",
    "    if len(count)<=1:\n",
    "       \n",
    "        return elements[0]\n",
    "   \n",
    "    elif len(features)==0:\n",
    "        return parent_node_class\n",
    "    #bulid tree\n",
    "    else:\n",
    "\n",
    "        #select parent node\n",
    "        parent_node_class=elements[np.argmax(count)]\n",
    "\n",
    "        #store information gain of each attributes\n",
    "        item_values=[]\n",
    "\n",
    "        #print parent node entropy\n",
    "      \n",
    "\n",
    "        # store inforamtion gain of each features in itme_values\n",
    "        for i in features:\n",
    "            item_values.append(informationGain(data,i,target))\n",
    "            \n",
    "            \n",
    "\n",
    "        #select the feature which is the biggest inforamation gain \n",
    "        index=np.argmax(item_values)\n",
    "        select=features[index]\n",
    "\n",
    "        \n",
    "        #create tree structre\n",
    "        tree={select:{}}\n",
    "\n",
    "        #features store features except selected feature\n",
    "        features=[i for i in features if i!=select]\n",
    "    \n",
    "        #draw child node of selected feature\n",
    "        for value in np.unique(data[select]):\n",
    "            sub_data=data.where(data[select]==value).dropna() #sub_data is data of selected features same with value\n",
    "          \n",
    "            subtree=ID3(sub_data, data, features, target, parent_node_class)#draw tree only using selected features\n",
    "            tree[select][value]=subtree #store tree result\n",
    "        return(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationGain(data,column,target):\n",
    "    entropy_total=entropy(data[target]) #calculate parent(root) entropy\n",
    "    elements,counts=np.unique(data[column],return_counts=True) # elements is unique value , count is number of each value\n",
    "\n",
    "    entropy_w=0 #entropy_w is weight entropy\n",
    "    for i in range(len(elements)):\n",
    "        en=entropy(data.where(data[column]==elements[i]).dropna()[target])#calculate entropy of each features\n",
    "       \n",
    "        entropy_w=entropy_w+counts[i]*en\n",
    "        \n",
    "    #calculate average\n",
    "    entropy_w=entropy_w/np.sum(counts)\n",
    "    \n",
    "\n",
    "    #calcualte information gain and return it\n",
    "    return entropy_total-entropy_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    elements,count=np.unique(data,return_counts=True)# elements is unique value , count is number of each value\n",
    "\n",
    "    #if number of unqiue target value(labeld value) -> entropy is 0\n",
    "    if len(count)==1:\n",
    "        en=0\n",
    "        \n",
    "    # else calculate entropy\n",
    "    else:\n",
    "        p1=count[0]/np.sum(count)\n",
    "        p2=count[1]/np.sum(count)\n",
    "        en=-(p1*np.log2(p1)+p2*np.log2(p2))\n",
    "    return en"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
